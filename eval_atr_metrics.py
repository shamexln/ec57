"""
eval_atr_metrics.py

ç”¨é€”ï¼š
- æ‰¹é‡è¿è¡Œ ECGAlg.exeï¼ˆå¯é€‰è·³è¿‡ï¼‰ï¼Œè¯»å–ç®—æ³•è¾“å‡º CSVï¼Œä¸ MIT-BIH çš„å‚è€ƒæ³¨é‡Š .atr æ¯”è¾ƒã€‚
- åœ¨ç»™å®šå®¹å·®ï¼ˆæ¯«ç§’ï¼‰å†…è¿›è¡Œä¸€å¯¹ä¸€æ‹ä½åŒ¹é…ï¼Œç»Ÿè®¡å››ç±»ï¼ˆN/V/F/Qï¼‰çš„æ··æ·†çŸ©é˜µä¸æŒ‡æ ‡ã€‚
- ç”Ÿæˆæ¯è®°å½•ä¸æ±‡æ€»çš„ CSVï¼›å¯é€‰ç”Ÿæˆç®€å• HTML æŠ¥å‘Šï¼ˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼‰ã€‚

ä¾èµ–ï¼šwfdb, numpy, pandas, matplotlibï¼ˆä»…å½“ --export-html æ—¶éœ€è¦ï¼‰

ç¤ºä¾‹ï¼š
python eval_atr_metrics.py --db "C:\\project\\mit-database" --exe .\\ECGAlg.exe --workdir . --outdir .\\metrics --all --tolerance-ms 100 --export-html
python eval_atr_metrics.py --db "C:\\project\\mit-database" --records 100 101 103 --skip-run --outdir .\\metrics --tolerance-ms 75
"""

from __future__ import annotations

import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import wfdb


# é»˜è®¤å››ç±»ï¼›å¦‚å¯ç”¨ --keep-L-class åˆ™åœ¨ main() ä¸­è¦†ç›–ä¸ºäº”ç±»
FOUR_CLASSES = ['N', 'V', 'F', 'Q']
# è¿è¡Œæ—¶æ§åˆ¶ï¼šæ˜¯å¦ä¿ç•™ L ç±»ï¼›è‹¥ä¸ä¿ç•™ï¼ŒL æ˜ å°„åˆ°ä½•ç±»ï¼ˆN æˆ– Qï¼‰
KEEP_L_CLASS: bool = False
MAP_L_TO: str = 'N'

# æ˜¯å¦å°†ç®—æ³•ç«¯ QRS å­ç±»åˆå¹¶ä¸º 'N'ï¼ˆé¿å…ä½ä¼° QRS æ£€æµ‹èƒ½åŠ›ï¼‰
MERGE_QRS: bool = True
# QRS å­ç±»é›†åˆï¼ˆåˆå¹¶ä¸º 'N'ï¼‰
QRS_SET = {'N', 'R', 'L', 'e', 'j', 'A', 'a', 'J', 'S'}


def list_records(db_dir: Path, records_cli: List[str] | None, use_all: bool) -> List[str]:
    if records_cli:
        return [str(r) for r in records_cli]
    if use_all:
        recs = []
        for p in db_dir.glob('*.hea'):
            recs.append(p.stem)
        recs.sort()
        return recs
    raise ValueError('å¿…é¡»ä½¿ç”¨ --records æˆ– --all æŒ‡å®šè¯„ä¼°çš„è®°å½•')


def rd_fs(db_dir: Path, record: str) -> float:
    sig, hdr = wfdb.rdsamp(str((db_dir / record).as_posix()))
    return float(hdr['fs'])


def read_ref_ann(db_dir: Path, record: str) -> Tuple[np.ndarray, List[str]]:
    ann = wfdb.rdann(str((db_dir / record).as_posix()), 'atr')
    return np.asarray(ann.sample, dtype=np.int64), list(ann.symbol)


def map_ref_symbol_to_four(sym: str) -> str:
    s = str(sym)
    # ä¾æ® AAMI ç®€åŒ–æ˜ å°„ï¼›è‹¥ä¿ç•™ L ç±»åˆ™å•ç‹¬è¾“å‡º 'L'
    n_set = {'N', 'R', 'e', 'j', 'A', 'a', 'J', 'S'}  # Sæ—å¹¶å…¥N
    v_set = {'V', 'E'}
    f_set = {'F'}
    if KEEP_L_CLASS and s == 'L':
        return 'L'
    if (not KEEP_L_CLASS) and s == 'L':
        return MAP_L_TO
    if s in n_set:
        return 'N'
    if s in v_set:
        return 'V'
    if s in f_set:
        return 'F'
    return 'Q'


def map_alg_symbol_to_four(sym: str) -> str:
    s = str(sym)
    # åˆå¹¶ç­–ç•¥ï¼šå½“ MERGE_QRS ä¸ºçœŸæ—¶ï¼ŒQRS_SET âˆª {'L'} ä¸€å¾‹æ˜ å°„ä¸º 'N'
    if MERGE_QRS and (s in QRS_SET):
        return 'N'
    # è‹¥æœªå¯ç”¨åˆå¹¶ï¼Œéµå¾ªåŸæœ‰æ˜ å°„ï¼›å½“ä¿ç•™ L ç±»æ—¶ï¼ŒLâ†’Lï¼›å¦åˆ™ Lâ†’MAP_L_TOï¼ˆé»˜è®¤Nï¼‰
    if s == 'L':
        return 'L' if KEEP_L_CLASS else MAP_L_TO
    mapping = {'N': 'N', 'V': 'V', 'F': 'F', 'Q': 'Q'}
    return mapping.get(s, 'N')  # æœªè¯†åˆ«é»˜è®¤ Nï¼ˆå¯æŒ‰éœ€æ”¹ä¸º 'Q'ï¼‰


def read_alg_csv(csv_path: Path, fs: float) -> Tuple[np.ndarray, List[str]]:
    """è¯»å–ç®—æ³• CSVï¼Œå…¼å®¹ä¸¤ç§è¡¨å¤´é£æ ¼ï¼š
    1) æ— è¡¨å¤´ï¼šindex,time,beat,hrï¼ˆé¦–è¡Œæ ‡é¢˜ï¼Œskiprows=1ï¼‰
    2) æœ‰è¡¨å¤´ï¼ˆå¯èƒ½å«ç©ºæ ¼ï¼‰ï¼šIndexPos,time,beat type,heart rate
       - è‹¥å­˜åœ¨ IndexPosï¼Œåˆ™ç›´æ¥ä½œä¸ºæ ·æœ¬ç´¢å¼•ï¼ˆunits: samplesï¼‰ï¼›
       - å¦åˆ™ä½¿ç”¨ timeï¼ˆç§’ï¼‰Ã— fs å››èˆäº”å…¥ã€‚
    """
    # å°è¯•è¯»å–ä¸ºæœ‰è¡¨å¤´
    def _normalize_cols(cols: List[str]) -> List[str]:
        return [str(c).strip().lower().replace(' ', '') for c in cols]

    df = None
    try:
        df0 = pd.read_csv(csv_path)
        if not df0.empty:
            cols = _normalize_cols(list(df0.columns))
            df0.columns = cols
            df = df0
    except Exception:
        df = None

    if df is None or df.empty or (
        not set(df.columns).intersection({'IndexPos', 'time', 'beat type', 'heart rate'})
    ):
        # å›é€€åˆ°æ— è¡¨å¤´æ ¼å¼
        df = pd.read_csv(csv_path, header=None, names=['index', 'time', 'beat', 'hr'], skiprows=1)
        if df.empty:
            return np.empty((0,), dtype=np.int64), []
        # æ ‡å‡†æ— è¡¨å¤´ï¼štime ä¸ºç§’
        times = pd.to_numeric(df['time'], errors='coerce').to_numpy()
        if np.isnan(times).any():
            raise ValueError(f'CSV {csv_path} time åˆ—åŒ…å« NaN')
        samples = np.rint(times * fs).astype(np.int64)
        beats_col = 'beat'
        beats = df[beats_col].astype(str).tolist()
        beats = [map_alg_symbol_to_four(b) for b in beats]
        return samples, beats

    # è§„èŒƒåŒ–è¡¨å¤´åˆ†æ”¯
    # ä¼˜å…ˆä½¿ç”¨ IndexPosï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ time Ã— fs
    if 'indexpos' in df.columns:
        idx = pd.to_numeric(df['indexpos'], errors='coerce').to_numpy()
        if np.isnan(idx).any():
            raise ValueError(f'CSV {csv_path} IndexPos åˆ—åŒ…å« NaN')
        samples = np.rint(idx).astype(np.int64)
    elif 'time' in df.columns:
        times = pd.to_numeric(df['time'], errors='coerce').to_numpy()
        if np.isnan(times).any():
            raise ValueError(f'CSV {csv_path} time åˆ—åŒ…å« NaN')
        samples = np.rint(times * fs).astype(np.int64)
    else:
        raise ValueError(f'CSV {csv_path} ç¼ºå°‘ IndexPos æˆ– time åˆ—')

    # ç¡®å®š beat åˆ—
    beat_col = 'beat' if 'beat' in df.columns else ('beattype' if 'beattype' in df.columns else None)
    if beat_col is None:
        raise ValueError(f'CSV {csv_path} ç¼ºå°‘ beat / beat type åˆ—')
    beats_raw = df[beat_col].astype(str).tolist()
    beats = [map_alg_symbol_to_four(b) for b in beats_raw]
    return samples, beats


def greedy_match(pred_samples: np.ndarray, true_samples: np.ndarray, tol_samples: int) -> Tuple[Dict[int, int], List[int], List[int]]:
    """è´ªå¿ƒæœ€è¿‘é‚»ä¸€å¯¹ä¸€åŒ¹é…ã€‚
    è¿”å›ï¼šmatch_mapï¼ˆpred_idx -> true_idxï¼‰ï¼Œunaligned_pred_idx åˆ—è¡¨ï¼Œunaligned_true_idx åˆ—è¡¨ã€‚
    é¢„æœŸ pred_samples/true_samples å·²å‡åºã€‚
    """
    i, j = 0, 0
    match: Dict[int, int] = {}
    used_true = set()
    while i < len(pred_samples) and j < len(true_samples):
        dt = pred_samples[i] - true_samples[j]
        if abs(dt) <= tol_samples:
            # å±€éƒ¨åœ¨çª—å†…ï¼Œå°è¯•åœ¨é™„è¿‘æ‰¾æœ€è¿‘çš„ true
            best_j = j
            best_abs = abs(dt)
            jj = j - 1
            while jj >= 0 and abs(pred_samples[i] - true_samples[jj]) <= tol_samples:
                if abs(pred_samples[i] - true_samples[jj]) < best_abs and jj not in used_true:
                    best_j = jj
                    best_abs = abs(pred_samples[i] - true_samples[jj])
                jj -= 1
            jj = j + 1
            while jj < len(true_samples) and abs(pred_samples[i] - true_samples[jj]) <= tol_samples:
                if abs(pred_samples[i] - true_samples[jj]) < best_abs and jj not in used_true:
                    best_j = jj
                    best_abs = abs(pred_samples[i] - true_samples[jj])
                jj += 1
            if best_j not in used_true:
                match[i] = best_j
                used_true.add(best_j)
            i += 1
            # j å‰ç§»åˆ°æœªä½¿ç”¨çš„æœ€è¿‘å¤„
            while j < len(true_samples) and j in used_true:
                j += 1
        elif dt < 0:
            # é¢„æµ‹åœ¨å‚è€ƒä¹‹å‰ï¼Œä¸”è¶…å‡ºçª—ï¼›è¯¥é¢„æµ‹ä¸å¯èƒ½ä¸å½“å‰ j åŒ¹é…
            i += 1
        else:
            # å‚è€ƒåœ¨é¢„æµ‹ä¹‹å‰ï¼Œä¸”è¶…å‡ºçª—ï¼›æ¨è¿›å‚è€ƒç´¢å¼•
            j += 1

    unaligned_pred = [pi for pi in range(len(pred_samples)) if pi not in match]
    unaligned_true = [ti for ti in range(len(true_samples)) if ti not in used_true]
    return match, unaligned_pred, unaligned_true


def confusion_and_metrics(y_true: List[str], y_pred: List[str],
                          unmatched_pred_classes: List[str], unmatched_true_classes: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:
    # æ··æ·†çŸ©é˜µï¼ˆè¡Œï¼šçœŸï¼Œåˆ—ï¼šé¢„æµ‹ï¼‰
    cm = pd.DataFrame(0, index=FOUR_CLASSES, columns=FOUR_CLASSES, dtype=int)
    for t, p in zip(y_true, y_pred):
        if t in FOUR_CLASSES and p in FOUR_CLASSES:
            cm.loc[t, p] += 1

    # å°†æœªé…å¯¹çš„é¢„æµ‹/å‚è€ƒè®¡å…¥ FP/FN
    extra_fp = {c: 0 for c in FOUR_CLASSES}
    for c in unmatched_pred_classes:
        if c in extra_fp:
            extra_fp[c] += 1
    extra_fn = {c: 0 for c in FOUR_CLASSES}
    for c in unmatched_true_classes:
        if c in extra_fn:
            extra_fn[c] += 1

    # é€ç±» TP/FP/FN/TNï¼ˆåŒ…å«æœªé…å¯¹å¸¦æ¥çš„ FP/FNï¼‰
    per_cls = []
    # æ€»ä½“æ ·æœ¬æ•°æŒ‰â€œå‚è€ƒæ‹æ€»æ•° + é¢å¤–è¢«ç®—æ³•è¯¯æŠ¥çš„æ‹æ•°ï¼ˆunmatched_predï¼‰â€ä¼°ç®—
    total = cm.values.sum() + sum(extra_fn.values()) + sum(extra_fp.values())
    for c in FOUR_CLASSES:
        TP = cm.loc[c, c]
        FP = (cm[c].sum() - TP) + extra_fp[c]
        FN = (cm.loc[c].sum() - TP) + extra_fn[c]
        TN = total - TP - FP - FN
        sens = TP / (TP + FN) if (TP + FN) > 0 else 0.0
        spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0
        prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0
        f1 = (2 * prec * sens / (prec + sens)) if (prec + sens) > 0 else 0.0
        acc = (TP + TN) / total if total > 0 else 0.0
        per_cls.append({'class': c, 'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN,
                        'Sensitivity': sens, 'Specificity': spec, 'PPV': prec, 'F1': f1, 'Accuracy': acc})
    per_cls_df = pd.DataFrame(per_cls)

    # å¾®/å®å¹³å‡ï¼ˆåŒ…å«æœªé…å¯¹çš„ FP/FNï¼‰
    TP_sum = sum(cm.loc[c, c] for c in FOUR_CLASSES)
    FP_sum = sum((cm[c].sum() - cm.loc[c, c]) + extra_fp[c] for c in FOUR_CLASSES)
    FN_sum = sum((cm.loc[c].sum() - cm.loc[c, c]) + extra_fn[c] for c in FOUR_CLASSES)
    TN_sum = total - TP_sum - FP_sum - FN_sum
    micro_recall = TP_sum / (TP_sum + FN_sum) if (TP_sum + FN_sum) > 0 else 0.0
    micro_prec = TP_sum / (TP_sum + FP_sum) if (TP_sum + FP_sum) > 0 else 0.0
    micro_f1 = (2 * micro_prec * micro_recall / (micro_prec + micro_recall)) if (micro_prec + micro_recall) > 0 else 0.0
    micro_acc = TP_sum / total if total > 0 else 0.0
    micro_spec = TN_sum / (TN_sum + FP_sum) if (TN_sum + FP_sum) > 0 else 0.0

    macro = per_cls_df[['Sensitivity', 'Specificity', 'PPV', 'F1', 'Accuracy']].mean().to_dict()
    summary = {
        'Micro_Recall': micro_recall,
        'Micro_Specificity': micro_spec,
        'Micro_Precision': micro_prec,
        'Micro_F1': micro_f1,
        'Micro_Accuracy': micro_acc,
        'Macro_Recall': macro['Sensitivity'],
        'Macro_Specificity': macro['Specificity'],
        'Macro_Precision': macro['PPV'],
        'Macro_F1': macro['F1'],
        'Macro_Accuracy': macro['Accuracy'],
        'Total': total,
    }
    return cm, per_cls_df, summary


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def maybe_run_algorithm(db_dir: Path, record: str, exe: Path, workdir: Path, outdir: Path, force: bool) -> Path:
    """è°ƒç”¨ wfdb_runner.py ç”Ÿæˆ CSVï¼Œè¿”å› CSV è·¯å¾„ã€‚"""
    csv_path = workdir / f'{record}.csv'
    if csv_path.exists() and not force:
        return csv_path
    # è°ƒç”¨åŒç›®å½•çš„ wfdb_runner.py
    import subprocess, sys
    runner = Path(__file__).parent / 'wfdb_runner.py'
    # ä¸ºä¸ C:\\project\\ECGAlg çš„å‚è€ƒå®ç°ä¸€è‡´ï¼š
    # - å°† trans.dat å†™å…¥å·¥ä½œç›®å½•ä¸‹ example_data å­ç›®å½•
    # - settings.ini ä½¿ç”¨ç›¸å¯¹è·¯å¾„ .\\example_data\\<record>_trans.dat
    example_outdir = workdir / 'example_data'
    example_outdir.mkdir(parents=True, exist_ok=True)
    cmd = [sys.executable, str(runner), '--db', str(db_dir), '--record', str(record), '--exe', str(exe), '--workdir', str(workdir), '--outdir', str(example_outdir), '--leadnum', '12']
    subprocess.run(cmd, check=True, cwd=Path(__file__).parent)
    return csv_path


def export_html_report(outdir: Path, overall_cm: pd.DataFrame, per_record_rows: List[Dict]):
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        import base64
        from io import BytesIO

        fig, ax = plt.subplots(figsize=(4, 4))
        im = ax.imshow(overall_cm.values, cmap='Blues')
        ax.set_xticks(range(len(FOUR_CLASSES)))
        ax.set_xticklabels(FOUR_CLASSES)
        ax.set_yticks(range(len(FOUR_CLASSES)))
        ax.set_yticklabels(FOUR_CLASSES)
        for i in range(len(FOUR_CLASSES)):
            for j in range(len(FOUR_CLASSES)):
                ax.text(j, i, str(overall_cm.values[i, j]), ha='center', va='center', color='black')
        ax.set_title('Overall Confusion Matrix (N/V/F/Q)')
        fig.colorbar(im, ax=ax, shrink=0.8)
        buf = BytesIO()
        plt.tight_layout()
        fig.savefig(buf, format='png', dpi=160)
        plt.close(fig)
        img_b64 = base64.b64encode(buf.getvalue()).decode('ascii')

        html = [
            '<html><head><meta charset="utf-8"><title>ECG Evaluation Report</title></head><body>',
            '<h2>Overall Confusion Matrix (N/V/F/Q)</h2>',
            f'<img src="data:image/png;base64,{img_b64}"/>',
            '<h2>Per-record Metrics (Micro Accuracy & F1)</h2>',
            '<table border="1" cellspacing="0" cellpadding="4">',
            '<tr><th>Record</th><th>Total</th><th>Micro Accuracy</th><th>Micro F1</th></tr>'
        ]
        for row in per_record_rows:
            html.append(f"<tr><td>{row['record']}</td><td>{row['Total']}</td><td>{row['Micro_Accuracy']:.4f}</td><td>{row['Micro_F1']:.4f}</td></tr>")
        html.append('</table></body></html>')
        out_html = outdir / 'report.html'
        out_html.write_text('\n'.join(html), encoding='utf-8')
    except Exception as e:
        (outdir / 'report_error.txt').write_text(str(e), encoding='utf-8')


def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description='æ‰¹é‡è¯„ä¼° ECGAlg.exe ä¸ MIT-BIH .atr çš„ä¸€è‡´æ€§ï¼ˆå››ç±» N/V/F/Qï¼‰')
    p.add_argument('--db', required=True, help='æœ¬åœ° WFDB æ•°æ®ç›®å½•ï¼Œä¾‹å¦‚ C:\\project\\mit-database')
    p.add_argument('--exe', default='.\\ECGAlg.exe', help='ECGAlg.exe è·¯å¾„ï¼›é…åˆ --skip-run å¯ä¸éœ€è¦')
    p.add_argument('--workdir', default='.', help='ç®—æ³•è¿è¡Œä¸ CSV è¾“å‡ºç›®å½•ï¼ˆrecord.csvï¼‰')
    p.add_argument('--outdir', default='.\\metrics', help='è¯„ä¼°è¾“å‡ºç›®å½•ï¼ˆCSV/HTMLï¼‰')
    p.add_argument('--records', nargs='*', help='æŒ‡å®šè®°å½•åˆ—è¡¨ï¼Œä¾‹å¦‚ 100 101 103')
    p.add_argument('--all', action='store_true', help='è¯„ä¼°ç›®å½•ä¸‹æ‰€æœ‰ .hea è®°å½•')
    p.add_argument('--tolerance-ms', type=int, default=100, help='åŒ¹é…å®¹å·®ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¸¸ç”¨ 100 æˆ– 75')
    p.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡è·‘ç®—æ³•ï¼ˆå³ä½¿ CSV å·²å­˜åœ¨ï¼‰')
    p.add_argument('--skip-run', action='store_true', help='è·³è¿‡è¿è¡Œç®—æ³•ï¼Œç›´æ¥è¯„ä¼°å·²å­˜åœ¨çš„ CSV')
    p.add_argument('--export-html', action='store_true', help='å¯¼å‡ºç®€å• HTML æŠ¥å‘Šï¼ˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼‰')
    p.add_argument('--keep-L-class', action='store_true', help='è¯„ä¼°æ—¶ä¿ç•™ L ä¸ºç‹¬ç«‹ç±»åˆ«ï¼ˆå½¢æˆ N/L/V/F/Q äº”ç±»ï¼‰')
    p.add_argument('--map-L-to', choices=['N', 'Q'], default='N', help='å½“ä¸ä¿ç•™ L ç±»æ—¶ï¼ŒL æ˜ å°„åˆ°å“ªä¸ªç±»åˆ«ï¼ˆé»˜è®¤ Nï¼‰')
    # æ–°å¢ï¼šæ˜¯å¦å°†ç®—æ³•ç«¯ QRS å­ç±»åˆå¹¶ä¸º Nï¼ˆé¿å…ä½ä¼° QRS èƒ½åŠ›ï¼‰
    p.add_argument('--merge-qrs', dest='merge_qrs', action='store_true', help="å°†ç®—æ³•ç«¯ QRS å­ç±» {'N','R','L','e','j','A','a','J','S'} åˆå¹¶ä¸º 'N'")
    p.add_argument('--no-merge-qrs', dest='merge_qrs', action='store_false', help='å…³é—­ç®—æ³•ç«¯ QRS å­ç±»åˆå¹¶ï¼ˆæŒ‰åŸå§‹ N/V/F/Q ä½¿ç”¨ï¼‰')
    p.set_defaults(merge_qrs=True)
    return p.parse_args(argv)


def main(argv: List[str] | None = None) -> int:
    args = parse_args(argv)
    db_dir = Path(args.db)
    exe = Path(args.exe)
    workdir = Path(args.workdir)
    outdir = Path(args.outdir)
    ensure_dir(outdir)
    per_record_dir = outdir / 'per_record'
    ensure_dir(per_record_dir)

    # è¿è¡ŒæœŸç±»åˆ«ä¸æ˜ å°„ç­–ç•¥
    global FOUR_CLASSES, KEEP_L_CLASS, MAP_L_TO, MERGE_QRS
    KEEP_L_CLASS = bool(args.keep_L_class)
    MAP_L_TO = str(args.map_L_to)
    MERGE_QRS = bool(args.merge_qrs)
    if KEEP_L_CLASS:
        FOUR_CLASSES = ['N', 'L', 'V', 'F', 'Q']

    records = list_records(db_dir, args.records, args.all)
    per_record_rows: List[Dict] = []
    overall_cm = pd.DataFrame(0, index=FOUR_CLASSES, columns=FOUR_CLASSES, dtype=int)

    for rec in records:
        try:
            fs = rd_fs(db_dir, rec)
            csv_path = workdir / f'{rec}.csv'
            if not args.skip_run:
                csv_path = maybe_run_algorithm(db_dir, rec, exe, workdir, outdir, args.force)
            if not csv_path.exists():
                print(f'âš  è·³è¿‡ {rec}ï¼šæœªæ‰¾åˆ° {csv_path}')
                continue

            # è¯»å–æ•°æ®
            ref_samples, ref_syms = read_ref_ann(db_dir, rec)
            ref_cls = [map_ref_symbol_to_four(s) for s in ref_syms]
            pred_samples, pred_syms = read_alg_csv(csv_path, fs)
            # å°†ç®—æ³•ç«¯ç¬¦å·æŒ‰ç­–ç•¥æ˜ å°„ä¸ºè¯„ä¼°ç±»åˆ«
            pred_cls = [map_alg_symbol_to_four(s) for s in pred_syms]

            # å‡åºæ’åº
            ref_order = np.argsort(ref_samples)
            ref_samples = ref_samples[ref_order]
            ref_cls = [ref_cls[i] for i in ref_order]
            pred_order = np.argsort(pred_samples)
            pred_samples = pred_samples[pred_order]
            pred_cls = [pred_cls[i] for i in pred_order]

            tol_samp = int(round(args.tolerance_ms * fs / 1000.0))
            match, unpred, untrue = greedy_match(pred_samples, ref_samples, tol_samp)

            # æ„é€ å·²é…å¯¹æ ·æœ¬çš„æ ‡ç­¾
            y_true, y_pred = [], []
            for pi, tj in match.items():
                y_true.append(ref_cls[tj])
                y_pred.append(pred_cls[pi])

            # æœªé…å¯¹çš„é¢„æµ‹æ‹ï¼ˆFPï¼‰å’Œå‚è€ƒæ‹ï¼ˆFNï¼‰çš„ç±»åˆ«
            unmatched_pred_classes = [pred_cls[i] for i in unpred]
            unmatched_true_classes = [ref_cls[j] for j in untrue]

            cm, per_cls_df, summary = confusion_and_metrics(
                y_true, y_pred, unmatched_pred_classes, unmatched_true_classes
            )
            overall_cm = overall_cm.add(cm, fill_value=0).astype(int)

            # ä¿å­˜æ¯è®°å½•ç»“æœ
            cm_path = per_record_dir / f'{rec}_confusion.csv'
            cls_path = per_record_dir / f'{rec}_per_class.csv'
            cm.to_csv(cm_path, encoding='utf-8')
            per_cls_df.to_csv(cls_path, index=False, encoding='utf-8')

            row = {'record': rec}
            row.update(summary)
            per_record_rows.append(row)

            print(f'âœ… {rec} å®Œæˆï¼šMicro_Acc={summary["Micro_Accuracy"]:.4f} Micro_F1={summary["Micro_F1"]:.4f} TotalPairs={summary["Total"]}')

        except Exception as e:
            print(f'âŒ {rec} å¤±è´¥ï¼š{e}')

    # æ±‡æ€»
    summary_df = pd.DataFrame(per_record_rows)
    sum_path = outdir / 'summary_metrics.csv'
    overall_cm_path = outdir / 'overall_confusion.csv'
    summary_df.to_csv(sum_path, index=False, encoding='utf-8')
    overall_cm.to_csv(overall_cm_path, encoding='utf-8')
    print(f'ğŸ“„ æ±‡æ€»å†™å…¥ï¼š{sum_path} ; æ··æ·†çŸ©é˜µï¼š{overall_cm_path}')

    # åœ¨ summary_metrics.csv æœ«è¿½åŠ  TOTAL è¡Œï¼ˆåŒ…å« Q/V çš„ TP/FP/FN åˆè®¡ä¸æ´¾ç”ŸæŒ‡æ ‡ï¼‰ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥é˜…
    try:
        def _tp_fp_fn_of(cls: str) -> tuple[int, int, int]:
            if cls not in overall_cm.index or cls not in overall_cm.columns:
                return 0, 0, 0
            TP = int(overall_cm.loc[cls, cls])
            FN = int(overall_cm.loc[cls].sum() - TP)
            FP = int(overall_cm[cls].sum() - TP)
            return TP, FP, FN

        TP_Q, FP_Q, FN_Q = _tp_fp_fn_of('Q')
        TP_V, FP_V, FN_V = _tp_fp_fn_of('V')
        Se_Q = (TP_Q / (TP_Q + FN_Q)) if (TP_Q + FN_Q) > 0 else 0.0
        PPV_Q = (TP_Q / (TP_Q + FP_Q)) if (TP_Q + FP_Q) > 0 else 0.0
        Se_V = (TP_V / (TP_V + FN_V)) if (TP_V + FN_V) > 0 else 0.0
        PPV_V = (TP_V / (TP_V + FP_V)) if (TP_V + FP_V) > 0 else 0.0

        total_row = {
            'record': 'TOTAL',
            'TP_Q': TP_Q, 'FP_Q': FP_Q, 'FN_Q': FN_Q, 'Se_Q': Se_Q, 'PPV_Q': PPV_Q,
            'TP_V': TP_V, 'FP_V': FP_V, 'FN_V': FN_V, 'Se_V': Se_V, 'PPV_V': PPV_V,
        }
        sum_df2 = pd.concat([summary_df, pd.DataFrame([total_row])], ignore_index=True)
        sum_path.write_text(sum_df2.to_csv(index=False), encoding='utf-8')
        print('â• å·²åœ¨ summary_metrics.csv è¿½åŠ  TOTAL åˆè®¡è¡Œï¼ˆQ/V æŒ‡æ ‡ï¼‰')
    except Exception as e:
        print(f'âš  è¿½åŠ  TOTAL è¡Œå¤±è´¥ï¼š{e}')

    if args.export_html:
        export_html_report(outdir, overall_cm, per_record_rows)
        print(f'ğŸ–¼ æŠ¥å‘Šï¼š{(outdir / "report.html").resolve()}')

    # ç”Ÿæˆâ€œæœ€ç»ˆæŠ¥å‘Šâ€æ–‡ä»¶ï¼ŒæŒ‰ç…§ç”¨æˆ·æ‰€éœ€æ ¼å¼ï¼š
    #            Q Se			Q +P			V Se			V +P
    # MIT-BIH    99.79			99.83			90.69			92.69
    try:
        # ä» overall_cm è®¡ç®—æ¯ç±»çš„ Se ä¸ +Pï¼ˆPPVï¼‰
        # overall_cm çš„è¡Œæ˜¯çœŸå€¼ï¼Œåˆ—æ˜¯é¢„æµ‹
        def _se_ppv_of(cls: str) -> tuple[float, float]:
            if cls not in overall_cm.index or cls not in overall_cm.columns:
                return 0.0, 0.0
            TP = int(overall_cm.loc[cls, cls])
            FN = int(overall_cm.loc[cls].sum() - TP)
            FP = int(overall_cm[cls].sum() - TP)
            se = (TP / (TP + FN)) if (TP + FN) > 0 else 0.0
            ppv = (TP / (TP + FP)) if (TP + FP) > 0 else 0.0
            return se, ppv

        q_se, q_ppv = _se_ppv_of('Q')
        v_se, v_ppv = _se_ppv_of('V')

        # æŒ‰ç™¾åˆ†æ•°ä¸¤ä½å°æ•°è¾“å‡º
        def _fmt(x: float) -> str:
            return f"{x * 100:.2f}"

        report_df = pd.DataFrame(
            [[
                'MIT-BIH',
                _fmt(q_se), _fmt(q_ppv),
                _fmt(v_se), _fmt(v_ppv),
            ]],
            columns=['', 'Q Se', 'Q +P', 'V Se', 'V +P']
        )
        final_path = outdir / 'final_report.csv'
        report_df.to_csv(final_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ æœ€ç»ˆæŠ¥å‘Šï¼š{final_path}')

        # ç”Ÿæˆå¯è¿½æº¯è¯¦æƒ… final_report_details.csvï¼ˆç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼›è®°å½•å®¹å·®/åˆå¹¶å¼€å…³ï¼‰
        def _fmt_pct(x: float) -> str:
            return f"{x * 100:.2f}"

        # è®¡ç®— TP/FP/FNï¼ˆä¸ä¸Šæ–¹ Se/PPV ä¸€è‡´ï¼‰
        def _tp_fp_fn_of2(cls: str) -> tuple[int, int, int]:
            if cls not in overall_cm.index or cls not in overall_cm.columns:
                return 0, 0, 0
            TP = int(overall_cm.loc[cls, cls])
            FN = int(overall_cm.loc[cls].sum() - TP)
            FP = int(overall_cm[cls].sum() - TP)
            return TP, FP, FN

        TP_Q, FP_Q, FN_Q = _tp_fp_fn_of2('Q')
        TP_V, FP_V, FN_V = _tp_fp_fn_of2('V')

        details_rows = [
            ['Run', 'Scope', 'MIT-BIH'],
            ['Run', 'Tolerance(ms)', str(int(args.tolerance_ms))],
            ['Run', 'MergeQRS', str(MERGE_QRS)],
            ['Q', 'TP', str(TP_Q)],
            ['Q', 'FP', str(FP_Q)],
            ['Q', 'FN', str(FN_Q)],
            ['Q', 'Se', _fmt_pct(q_se)],
            ['Q', 'PPV', _fmt_pct(q_ppv)],
            ['V', 'TP', str(TP_V)],
            ['V', 'FP', str(FP_V)],
            ['V', 'FN', str(FN_V)],
            ['V', 'Se', _fmt_pct(v_se)],
            ['V', 'PPV', _fmt_pct(v_ppv)],
        ]
        details_df = pd.DataFrame(details_rows, columns=['Section', 'Item', 'Value'])
        details_path = outdir / 'final_report_details.csv'
        details_df.to_csv(details_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ è¯¦æƒ…æŠ¥å‘Šï¼š{details_path}')
    except Exception as e:
        (outdir / 'final_report_error.txt').write_text(str(e), encoding='utf-8')

    return 0


if __name__ == '__main__':
    import sys
    raise SystemExit(main(sys.argv[1:]))
