r"""
bxb_runner.py â€” ä½¿ç”¨ WFDB å®˜æ–¹æ¯”è¾ƒå™¨ bxb.exe è¿›è¡Œé€æ‹å¯¹æ¯”ï¼ˆBeat-by-beat comparisonï¼‰

åŠŸèƒ½ï¼š
- æ‰¹é‡ï¼ˆ--all æˆ– --recordsï¼‰ä¸ºæ¯æ¡è®°å½•ç¡®ä¿ç”Ÿæˆç®—æ³• CSV ä¸ WFDB æ³¨é‡Š <record>.bioï¼›
- è‡ªåŠ¨è°ƒç”¨ bxb.exe ä¸å‚è€ƒæ³¨é‡Š atr é€æ‹å¯¹æ¯”ï¼›
- è§£æ bxb.exe çš„æ ‡å‡†è¾“å‡ºï¼ŒæŠ½å– TP/FP/FNï¼ˆåŒ¹é…/è¯¯æŠ¥/æ¼æŠ¥ï¼‰ç­‰å…³é”®ç»Ÿè®¡ï¼›
- å°†æ¯è®°å½•ä¸æ€»ä½“çš„ç»Ÿè®¡å†™å…¥ metrics_bxb\summary.csvã€‚

æ³¨æ„ï¼š
- è¢«æµ‹æ³¨é‡Šæ‰©å±•å›ºå®šä¸º bioï¼ˆä¸ wfdb_runner.py çš„ --to-ann è¾“å‡ºä¸€è‡´ï¼‰ã€‚
- å‚è€ƒæ‰©å±•å›ºå®šä¸º atrï¼›å®¹å·®ç”± --tolerance-ms æŒ‡å®šï¼ˆé»˜è®¤ 100 msï¼‰ã€‚
- éœ€è¦å·²å®‰è£… wfdbï¼ˆä»…ç”¨äºè¯»å– fsï¼Œç”Ÿæˆ .bio æ—¶ä½¿ç”¨ï¼‰ã€‚
"""

from __future__ import annotations

import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import wfdb


def list_records(db_dir: Path, records_cli: List[str] | None, use_all: bool) -> List[str]:
    if records_cli:
        return [str(r) for r in records_cli]
    
    recs = []
    if use_all:
        recs = sorted([p.stem for p in db_dir.glob('*.hea')])
    
    # å¦‚æœæ²¡æ‰¾åˆ°ä¸”æ²¡æ˜¾å¼æŠ¥é”™ï¼Œè¿”å›ç©ºåˆ—è¡¨
    return recs


def rd_fs(db_dir: Path, record: str) -> float:
    sig, hdr = wfdb.rdsamp(str((db_dir / record).as_posix()))
    return float(hdr['fs'])


def csv_to_bio(record: str, csv_path: Path, out_dir: Path, fs: float) -> Path:
    """æŠŠç®—æ³• CSV è½¬ä¸º WFDB æ³¨é‡Š <record>.bioï¼ˆå…¼å®¹ä¸¤ç§åˆ—æ ¼å¼ï¼‰ã€‚
    åˆ—æ ¼å¼Aï¼ˆæ— è¡¨å¤´ï¼‰ï¼šindex,time,beat,hrï¼ˆé¦–è¡Œæ ‡é¢˜ï¼Œskiprows=1ï¼‰ã€‚
    åˆ—æ ¼å¼Bï¼ˆæœ‰è¡¨å¤´ï¼‰ï¼šIndexPos,time,beat type,heart rateã€‚
    ä½¿ç”¨ IndexPos ä½œä¸ºæ ·æœ¬ç´¢å¼•
    æœªè¯†åˆ« beat é»˜è®¤æ˜ å°„ä¸º 'N'ï¼›
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    bio_path = out_dir / f'{record}.bio'

    # å°è¯•æœ‰è¡¨å¤´
    def _normalize(cols: List[str]) -> List[str]:
        return [str(c).strip().lower().replace(' ', '') for c in cols]

    df = None
    try:
        df0 = pd.read_csv(csv_path)
        cols = _normalize(list(df0.columns))
        df0.columns = cols
        df = df0
    except Exception:
        df = None

    samples: np.ndarray
    beats: List[str]
    if df is None or df.empty or not set(df.columns).intersection({'indexpos', 'time', 'beat', 'beattype'}):
        # æ— è¡¨å¤´æ ¼å¼
        df = pd.read_csv(csv_path, header=None, names=['index', 'time', 'beat', 'hr'], skiprows=1)
        if df.empty:
            # å†™ä¸€ä¸ªç©ºæ³¨é‡Šæ–‡ä»¶ï¼ˆä»¥å… bxb æŠ¥é”™ï¼‰ï¼›ä½†é€šå¸¸è¿™æ„å‘³ç€ç®—æ³•æ— è¾“å‡º
            wfdb.wrann(record, 'bio', sample=np.asarray([], dtype=np.int64), symbol=[])
            return bio_path
        idx = pd.to_numeric(df['indexpos'], errors='coerce').to_numpy()
        samples = np.rint(idx).astype(np.int64)
        beats = df['beat'].astype(str).tolist()
    else:
        if 'indexpos' in df.columns:
            idx = pd.to_numeric(df['indexpos'], errors='coerce').to_numpy()
            samples = np.rint(idx).astype(np.int64)
        else:
            raise ValueError(f'{csv_path} ç¼ºå°‘ IndexPos æˆ– time åˆ—')
        bc = 'beat' if 'beat' in df.columns else ('beattype' if 'beattype' in df.columns else None)
        if bc is None:
            raise ValueError(f'{csv_path} ç¼ºå°‘ beat / beat type åˆ—')
        beats = df[bc].astype(str).tolist()

    # å°† L åŸæ ·æˆ–æ˜ å°„ï¼Ÿè¿™é‡Œä¸æ”¹å˜ labelï¼Œä»…ä½œä¸ºäº‹ä»¶ï¼›bxb åªå…³å¿ƒæ—¶é—´å¯¹é½
    symbols = beats

    # æ’åºå¹¶å†™å‡ºï¼ˆæ˜ç¡®å†™å…¥ bio_path æŒ‡å®šç›®å½•ï¼‰
    order = np.argsort(samples)
    samples = samples[order]
    symbols = [symbols[i] for i in order]
    # wfdb.wrann ä¼šä»¥å½“å‰å·¥ä½œç›®å½•ä¸ºåŸºå‡†å°† <record>.<ext> å†™å…¥ç£ç›˜ã€‚
    # ä¸ºç¡®ä¿å†™åˆ° bio_path æŒ‡å®šç›®å½•ï¼Œåˆ‡æ¢ä¸´æ—¶å·¥ä½œç›®å½•å†å†™å…¥ã€‚
    import os
    old_cwd = os.getcwd()
    try:
        os.chdir(str(out_dir))
        wfdb.wrann(record, 'bio', sample=samples, symbol=symbols)
    finally:
        os.chdir(old_cwd)
    return bio_path


def run_bxb(bxb_exe: Path, workdir: Path, record: str, wfdbpath: str, test_ext: str = 'bio') -> Tuple[str, int]:
    """è°ƒç”¨ bxb.exeï¼Œè¿”å›æ ‡å‡†è¾“å‡ºæ–‡æœ¬ä¸é€€å‡ºç ã€‚
    ä¼šåœ¨å­è¿›ç¨‹ç¯å¢ƒä¸­è®¾ç½® WFDBPATHï¼Œä»¥ä¾¿ bxb èƒ½æ‰¾åˆ° .hea/.dat/.atr ä¸ æµ‹è¯•æ³¨é‡Šï¼ˆæ‰©å±•åç”± test_ext æŒ‡å®šï¼‰ã€‚
    ä¾‹å¦‚ï¼štest_ext='nbio' æˆ– 'vbio'ã€‚
    """
    import os
    cmd = [str(bxb_exe), '-r', record, '-a', 'atr', str(test_ext), '-l' , 'sta.txt', 'ansta.txt']
    env = os.environ.copy()
    # Ensure workdir is absolute
    abs_workdir = str(workdir.resolve())
    # WFDBPATH should prioritize workdir so bxb writes output there
    env['WFDBPATH'] = f"{abs_workdir};{wfdbpath}"
    proc = subprocess.run(cmd, cwd=abs_workdir, capture_output=True, text=True, encoding='utf-8', errors='replace', env=env)
    stdout = proc.stdout + (proc.stderr or '')
    return stdout, proc.returncode


def _read_bio_as_df(workdir: Path, record: str) -> pd.DataFrame:
    """è¯»å– workdir ä¸‹ <record>.bio ä¸º DataFrame(sample, symbol)ã€‚è‹¥ä¸å­˜åœ¨è¿”å›ç©º DFã€‚"""
    bio_path = workdir / f'{record}.bio'
    if not bio_path.exists():
        return pd.DataFrame({'sample': pd.Series(dtype='int64'), 'symbol': pd.Series(dtype='object')})
    ann = wfdb.rdann(str((workdir / record).as_posix()), 'bio')
    df = pd.DataFrame({'sample': np.asarray(ann.sample, dtype=np.int64), 'symbol': list(ann.symbol)})
    # æ’åºï¼Œå»é™¤ NaN/ç©ºç¬¦å·
    df = df.dropna(subset=['sample', 'symbol'])
    df = df.sort_values('sample').reset_index(drop=True)
    return df


def _write_bio_from_df(workdir: Path, record: str, df: pd.DataFrame, out_ext: str) -> Path:
    """å°† DataFrame å†™ä¸º WFDB æ³¨é‡Š <record>.<out_ext> åˆ° workdirã€‚"""
    samples = np.asarray(pd.to_numeric(df['sample'], errors='coerce').dropna().round().astype(np.int64)) if not df.empty else np.asarray([], dtype=np.int64)
    symbols = df['symbol'].astype(str).tolist() if not df.empty else []
    import os
    old_cwd = os.getcwd()
    try:
        os.chdir(str(workdir))
        wfdb.wrann(record, out_ext, sample=samples, symbol=symbols)
    finally:
        os.chdir(old_cwd)
    return workdir / f'{record}.{out_ext}'


def parse_bxb_stdout(text: str) -> Dict[str, int | float]:
    """ä» bxb è¾“å‡ºä¸­æŠ½å–å…³é”®ç»Ÿè®¡ã€‚ä¸åŒç‰ˆæœ¬æ ¼å¼ç•¥æœ‰åŒºåˆ«ï¼Œè¿™é‡Œä½¿ç”¨å…³é”®å­—æå–ï¼š
    è¿”å›é”®ï¼šTPï¼ˆåŒ¹é…ï¼‰ã€FPï¼ˆè¯¯æŠ¥ï¼‰ã€FNï¼ˆæ¼æŠ¥ï¼‰ã€TOTAL_REFã€TOTAL_TESTã€‚
    è‹¥æ— æ³•è¯†åˆ«ï¼Œè¿”å›ç©ºå­—å…¸ã€‚"""
    # ç®€å•å¯å‘å¼è§£æ
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    stats: Dict[str, int | float] = {}
    for ln in lines:
        low = ln.lower()
        # å¸¸è§å…³é”®å­—ï¼šmatched / false positives / false negatives / total annotated / total detected
        if 'matched' in low and any(ch.isdigit() for ch in low):
            # e.g., "Matched annotations: 2274"
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TP'] = int(digits[-1])
        elif 'false positive' in low:
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['FP'] = int(digits[-1])
        elif 'false negative' in low:
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['FN'] = int(digits[-1])
        elif ('total' in low and 'reference' in low) or ('total' in low and 'annotated' in low):
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TOTAL_REF'] = int(digits[-1])
        elif ('total' in low and 'test' in low) or ('total' in low and 'detected' in low):
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TOTAL_TEST'] = int(digits[-1])
    return stats


def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description='ä½¿ç”¨ bxb.exe å¯¹ç®—æ³• .bio ä¸å‚è€ƒ .atr åšé€æ‹å¯¹æ¯”ï¼ˆæ‰¹é‡ï¼‰')
    p.add_argument('--db', required=True, help='MIT/PhysioNet æ•°æ®ç›®å½•ï¼Œå¦‚ C\\project\\mit-database')
    p.add_argument('--exe', default='.\\ECGAlg.exe', help='ECGAlg.exe è·¯å¾„ï¼ˆç”¨äºç”Ÿæˆ CSV/.bioï¼‰')
    p.add_argument('--bxb', required=True, help='bxb.exe è·¯å¾„ï¼Œä¾‹å¦‚ C\\project\\wfdb-10.7.0\\build\\bin\\bxb.exe')
    p.add_argument('--workdir', default='.', help='å·¥ä½œç›®å½•ï¼ˆç”Ÿæˆ CSV/.bio ä¸æ‰§è¡Œ bxb çš„ä½ç½®ï¼‰')
    p.add_argument('--outdir', default='.\\metrics_bxb', help='è¾“å‡ºç›®å½•ï¼ˆsummary.csvï¼‰')
    p.add_argument('--records', nargs='*', help='æŒ‡å®šè®°å½•åˆ—è¡¨ï¼Œå¦‚ 100 101 103')
    p.add_argument('--all', action='store_true', help='å¯¹ç›®å½•ä¸‹æ‰€æœ‰ .hea è®°å½•æ‰§è¡Œ')
    p.add_argument('--tolerance-ms', type=int, default=100, help='bxb å®¹å·®ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 100')
    p.add_argument('--force', action='store_true', help='å³ä½¿å·²æœ‰ CSV/.bio ä¹Ÿå¼ºåˆ¶é‡è·‘ç®—æ³•ä¸é‡å†™ .bio')
    # QRS åˆå¹¶å¼€å…³ï¼šé»˜è®¤å¼€å¯ï¼Œå¯ç”¨ --no-merge-qrs å…³é—­
    p.add_argument('--merge-qrs', dest='merge_qrs', action='store_true', help="å°† QRS å­ç±» {'N','R','L','e','j','A','a','J','S'} åˆå¹¶ä¸º 'N'ï¼ˆç”¨äº nbio ç”Ÿæˆï¼‰")
    p.add_argument('--no-merge-qrs', dest='merge_qrs', action='store_false', help='å…³é—­ QRS å­ç±»åˆå¹¶ï¼Œä»…ä¿ç•™åŸå§‹ N ä½œä¸º Q åˆ—')
    p.set_defaults(merge_qrs=True)
    return p.parse_args(argv)


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def main(argv: List[str] | None = None) -> int:
    args = parse_args(argv)
    db_dir = Path(args.db)
    bxb_exe = Path(args.bxb)
    workdir = Path(args.workdir)
    outdir = Path(args.outdir)
    ensure_dir(outdir)
    ensure_dir(workdir)

    records = list_records(db_dir, args.records, args.all)

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®°å½•ï¼Œå°è¯•è‡ªåŠ¨å¯»æ‰¾ï¼ˆæ»¡è¶³â€œå¦‚æœæ²¡æœ‰å¤„ç†æ•°æ®ï¼Œå…ˆä»£ç è‡ªå·±è°ƒç”¨â€çš„è¦æ±‚ï¼‰
    if not records:
        print(f"â„¹ åœ¨ {db_dir} æœªæ‰¾åˆ°è®°å½•ã€‚å°è¯•å¯»æ‰¾å…¶å®ƒæ•°æ®ç›®å½•...")
        # å¸¸è§ä½ç½®æ¢æµ‹
        candidates = [
            Path("C:/project/mit-database"),
            db_dir.parent,
            Path(".")
        ]
        for cand in candidates:
            if cand.exists() and list(cand.glob("*.hea")):
                print(f"ğŸ” è‡ªåŠ¨å‘ç°æ•°æ®ç›®å½•: {cand}")
                db_dir = cand
                records = list_records(db_dir, None, True)
                break
    
    if not records:
        print(f"âš  æ— æ³•æ‰¾åˆ°ä»»ä½• .hea è®°å½•ã€‚è¯·é€šè¿‡ --db æŒ‡å®šæ­£ç¡®çš„æ•°æ®ç›®å½•ã€‚")
        return 0

    # 1.5) å°†æœ€ç»ˆç¡®å®šçš„ MIT æ•°æ®ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä¸å«å­ç›®å½•ï¼‰å¤åˆ¶åˆ° workdir å’Œ outdir
    # ä»¥ä¾¿ bxb åœ¨å·¥ä½œç›®å½•ç›´æ¥æ‰¾åˆ° <record>.hea/.dat/.atrï¼Œä»¥åŠåœ¨ outdir å­˜æ”¾ç»“æœ
    for target in [workdir, outdir]:
        try:
            print(f"ğŸ“‚ æ­£åœ¨å¤åˆ¶æ•°æ®æ–‡ä»¶åˆ° {target} ...")
            for p in db_dir.iterdir():
                if p.is_file():
                    dst = target / p.name
                    if not dst.exists():
                        # ä½¿ç”¨ read_bytes/write_bytes ç®€å•å®ç°å¤åˆ¶
                        dst.write_bytes(p.read_bytes())
        except Exception as e:
            print(f"âš  å¤åˆ¶æ•°æ®æ–‡ä»¶åˆ° {target} æ—¶å‡ºé”™ï¼š{e}")

    rows: List[Dict] = []

    # åœ¨å¼€å§‹å¾ªç¯å‰ï¼Œæ¸…ç†æ—§çš„ç»Ÿè®¡æ–‡ä»¶ï¼Œä»¥ç¡®ä¿ bxb é‡æ–°å¼€å§‹ç´¯åŠ ï¼ˆå‚è€ƒ process_atr.pyï¼‰
    for f in ["sta.txt", "ansta.txt"]:
        f_path = workdir / f
        if f_path.exists():
            f_path.unlink()

    # é€æ¡è®°å½•å¤„ç†
    for rec in records:
        try:
            fs = rd_fs(db_dir, rec)
            csv_path = workdir / f'{rec}.csv'
            bio_path = workdir / f'{rec}.bio'

            # è‹¥ç¼º CSV æˆ–å¼ºåˆ¶ï¼Œå…ˆç”¨ç°æœ‰è¿è¡Œå™¨äº§ç”Ÿ CSV
            if args.force or (not csv_path.exists()):
                # å¤ç”¨ wfdb_runner.py ä»¥ä¿æŒä¸€è‡´
                runner = Path(__file__).parent / 'wfdb_runner.py'
                cmd = [
                    'python', str(runner), '--db', str(db_dir), '--record', rec,
                    '--exe', str(Path(args.exe)), '--workdir', str(workdir), '--outdir', str(workdir), '--leadnum', '12'
                ]
                subprocess.run(cmd, check=True)

            # ç”Ÿæˆ .bioï¼ˆè‹¥ç¼ºæˆ–å¼ºåˆ¶ï¼‰ã€‚è‹¥ db_dir ä¸‹å·²æœ‰åŒå .bioï¼Œä¹Ÿä¼šå¤åˆ¶åˆ° workdirã€‚
            if not bio_path.exists():
                src_bio_in_db = db_dir / f'{rec}.bio'
                if src_bio_in_db.exists():
                    bio_path.write_bytes(src_bio_in_db.read_bytes())
                elif args.force or (not bio_path.exists()):
                    csv_to_bio(rec, csv_path, workdir, fs)
            elif args.force:
                # å¼ºåˆ¶é‡å†™
                csv_to_bio(rec, csv_path, workdir, fs)

            # åœ¨è°ƒç”¨ bxb å‰æ˜¾å¼æ£€æŸ¥ .bio æ˜¯å¦å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨åˆ™æŠ¥é”™å¹¶ç»™å‡ºæç¤º
            bio_exists = bio_path.exists()
            if not bio_exists:
                rows.append({'record': rec, 'exit_code': -2, 'error': f'ç¼ºå°‘ {bio_path}ï¼Œè¯·ç¡®è®¤å·²ç”Ÿæˆ CSV æˆ–æä¾›ç°æˆ .bio'})
                continue

            # è°ƒç”¨ bxb.exeï¼Œå¹¶æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆå­è¿›ç¨‹ cwd ä¸ WFDBPATH ä¼šè¢«è®¾ç½®ï¼‰
            # Use absolute path for db_dir in WFDBPATH
            abs_db_dir = str(db_dir.resolve())
            wfdbpath = f"{abs_db_dir}"
            print(f"[bxb] cwd={workdir}")
            # 1) æ•´ä½“ç»Ÿè®¡ï¼ˆä¸åŒºåˆ†ç±»åˆ«ï¼Œæµ‹è¯•æ‰©å±•ä½¿ç”¨åŸ bioï¼‰
            stdout_all, code_all = run_bxb(bxb_exe, workdir, rec, wfdbpath=wfdbpath, test_ext='bio')
            stats_all = parse_bxb_stdout(stdout_all)

            # å¡«å……åˆ° rows ä¸­
            row = {'record': rec, 'exit_code': code_all}
            row.update(stats_all)
            rows.append(row)
        except Exception as e:
            rows.append({'record': rec, 'error': str(e), 'exit_code': -1})

    # å†™å‡ºæ±‡æ€»
    if not rows:
        print("âš  æ²¡æœ‰å¤„ç†ä»»ä½•è®°å½•ï¼Œè·³è¿‡æ±‡æ€»ã€‚")
        return 0

    df = pd.DataFrame(rows)
    df = df.sort_values('record')
    # å…ˆå†™å‡ºåŸå§‹æ˜ç»†
    (outdir / 'summary.csv').write_text(df.to_csv(index=False), encoding='utf-8')
    print(f'ğŸ“„ bxb æ±‡æ€»å†™å…¥ï¼š{outdir / "summary.csv"}')

    # å¤„ç†ç´¯åŠ çš„ sta.txt
    sta_txt = workdir / "sta.txt"
    if sta_txt.exists():
        try:
            print(f"æ­£åœ¨è¯»å–ç´¯åŠ ç»Ÿè®¡æ–‡ä»¶: {sta_txt}")
            # è¯»å– sta.txt
            df_sta = pd.read_csv(sta_txt, sep=r'\s+')
            
            # å°†åˆ—åæ˜ å°„ä¸º Excel åˆ—å (A, B, C...)
            def get_excel_col_name(n):
                res = ""
                while n > 0:
                    n, rem = divmod(n - 1, 26)
                    res = chr(65 + rem) + res
                return res
            
            # é‡å‘½ååˆ—åä¸º A, B, C...
            excel_cols = [get_excel_col_name(i + 1) for i in range(len(df_sta.columns))]
            df_sta.columns = excel_cols

            # å¯¹æ€§èƒ½æŒ‡æ ‡åˆ—æ±‚å¹³å‡ï¼ŒåŠ åœ¨æœ€åä¸€è¡Œï¼ˆæ’é™¤ - å’Œ 0ï¼‰
            # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œä½¿ç”¨ Excel åˆ—åï¼šM, N, O, P, Q ç­‰
            # M: Q Se, N: Q +P, O: V Se, P: V +P, Q: V FPR
            cols_to_avg = ['M', 'N', 'O', 'P', 'Q']
            # ç¡®ä¿è¿™äº›åˆ—å­˜åœ¨
            existing_cols = [c for c in cols_to_avg if c in df_sta.columns]
            if existing_cols:
                # å¤åˆ¶ä¸€ä»½ç”¨äºè®¡ç®—ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®å±•ç¤º
                df_calc = df_sta[existing_cols].copy()
                for c in existing_cols:
                    # è½¬æ¢ä¸ºæ•°å€¼ï¼Œéæ•°å€¼è½¬æ¢ä¸º NaN
                    df_calc[c] = pd.to_numeric(df_calc[c], errors='coerce')
                    # å°† 0 æ›¿æ¢ä¸º NaNï¼Œä»¥ä¾¿ mean() æ’é™¤å®ƒä»¬
                    df_calc[c] = df_calc[c].replace(0, np.nan)
                
                avg_values = df_calc.mean()
                # åˆ›å»ºå¹³å‡å€¼è¡Œï¼Œéç›®æ ‡åˆ—è®¾ä¸ºç©ºæˆ–ç‰¹å®šæ ‡è¯†
                avg_row = {col: '' for col in df_sta.columns}
                if 'A' in df_sta.columns: # A åˆ—é€šå¸¸æ˜¯ Record
                    avg_row['A'] = 'AVERAGE'
                
                for c in existing_cols:
                    avg_row[c] = avg_values[c]
                
                df_sta = pd.concat([df_sta, pd.DataFrame([avg_row])], ignore_index=True)
                print(f"ğŸ“Š å·²åœ¨ sta_report.csv è¿½åŠ  {', '.join(existing_cols)} çš„å¹³å‡å€¼è¡Œ")

            # æœ€åå°†ç‰¹å®šåˆ—åé‡å‘½åä¸ºå«ä¹‰å
            rename_map = {
                'M': 'Q Se',
                'N': 'Q +P',
                'O': 'V Se',
                'P': 'V +P'
            }
            df_sta.rename(columns=rename_map, inplace=True)
            print(f"ğŸ·ï¸ å·²é‡å‘½ååˆ—: {rename_map}")

            output_report = outdir / "sta_report.csv"
            df_sta.to_csv(output_report, index=False, encoding='utf-8')
            print(f"âœ… ç´¯åŠ ç»Ÿè®¡æŠ¥å‘Šå·²å¯¼å‡ºè‡³: {output_report}")
        except Exception as e:
            print(f"âš  è§£æ sta.txt å‡ºé”™: {e}")
    else:
        print(f"âš  æœªæ‰¾åˆ°ç´¯åŠ æ–‡ä»¶ {sta_txt}ï¼Œè¯·æ£€æŸ¥ bxb.exe æ˜¯å¦ç”Ÿæˆäº†è¯¥æ–‡ä»¶ã€‚")

    # åœ¨ summary.csv ä¸­è¿½åŠ  TOTAL è¡Œï¼ˆåˆè®¡ï¼‰
    try:
        def _num_series(name: str) -> pd.Series:
            return pd.to_numeric(df.get(name, pd.Series(dtype=float)), errors='coerce').fillna(0)

        TP_sum = int(_num_series('TP').sum())
        FP_sum = int(_num_series('FP').sum())
        FN_sum = int(_num_series('FN').sum())
        Se_sum = (TP_sum / (TP_sum + FN_sum)) if (TP_sum + FN_sum) > 0 else 0.0
        PPV_sum = (TP_sum / (TP_sum + FP_sum)) if (TP_sum + FP_sum) > 0 else 0.0

        TP_N_sum = int(_num_series('TP_N').sum())
        FP_N_sum = int(_num_series('FP_N').sum())
        FN_N_sum = int(_num_series('FN_N').sum())
        Se_N_sum = (TP_N_sum / (TP_N_sum + FN_N_sum)) if (TP_N_sum + FN_N_sum) > 0 else 0.0
        PPV_N_sum = (TP_N_sum / (TP_N_sum + FP_N_sum)) if (TP_N_sum + FP_N_sum) > 0 else 0.0

        TP_V_sum = int(_num_series('TP_V').sum())
        FP_V_sum = int(_num_series('FP_V').sum())
        FN_V_sum = int(_num_series('FN_V').sum())
        Se_V_sum = (TP_V_sum / (TP_V_sum + FN_V_sum)) if (TP_V_sum + FN_V_sum) > 0 else 0.0
        PPV_V_sum = (TP_V_sum / (TP_V_sum + FP_V_sum)) if (TP_V_sum + FP_V_sum) > 0 else 0.0

        total_row = {
            'record': 'TOTAL',
            'TP': TP_sum, 'FP': FP_sum, 'FN': FN_sum, 'Se': Se_sum, 'PPV': PPV_sum,
            'TP_N': TP_N_sum, 'FP_N': FP_N_sum, 'FN_N': FN_N_sum, 'Se_N': Se_N_sum, 'PPV_N': PPV_N_sum,
            'TP_V': TP_V_sum, 'FP_V': FP_V_sum, 'FN_V': FN_V_sum, 'Se_V': Se_V_sum, 'PPV_V': PPV_V_sum,
        }
        df2 = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)
        (outdir / 'summary.csv').write_text(df2.to_csv(index=False), encoding='utf-8')
        print('â• å·²åœ¨ summary.csv è¿½åŠ  TOTAL åˆè®¡è¡Œ')
    except Exception as e:
        print(f'âš  è¿½åŠ  TOTAL è¡Œå¤±è´¥ï¼š{e}')

    # ç”Ÿæˆâ€œæœ€ç»ˆæŠ¥å‘Šâ€ï¼šæŒ‰ bxb -l N ä¸ -l V åˆ†åˆ«ç»Ÿè®¡çš„ç»“æœæ±‡æ€»ï¼Œ
    # è¾“å‡ºåˆ° outdir/final_report.csvï¼Œè¡¨å¤´ä¸ eval_atr_metrics.py ä¸€è‡´ï¼š
    # "", Q Se, Q +P, V Se, V +P
    try:
        # æ±‡æ€» N ç±»ï¼ˆå¯¹åº” Q åˆ—ï¼‰
        TP_N_sum = int(pd.to_numeric(df.get('TP_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FP_N_sum = int(pd.to_numeric(df.get('FP_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FN_N_sum = int(pd.to_numeric(df.get('FN_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())

        q_se = (TP_N_sum / (TP_N_sum + FN_N_sum)) if (TP_N_sum + FN_N_sum) > 0 else 0.0
        q_ppv = (TP_N_sum / (TP_N_sum + FP_N_sum)) if (TP_N_sum + FP_N_sum) > 0 else 0.0

        # æ±‡æ€» V ç±»
        TP_V_sum = int(pd.to_numeric(df.get('TP_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FP_V_sum = int(pd.to_numeric(df.get('FP_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FN_V_sum = int(pd.to_numeric(df.get('FN_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())

        v_se = (TP_V_sum / (TP_V_sum + FN_V_sum)) if (TP_V_sum + FN_V_sum) > 0 else 0.0
        v_ppv = (TP_V_sum / (TP_V_sum + FP_V_sum)) if (TP_V_sum + FP_V_sum) > 0 else 0.0

        def _fmt(x: float) -> str:
            return f"{x * 100:.2f}"

        report_df = pd.DataFrame([
            ['MIT-BIH', _fmt(q_se), _fmt(q_ppv), _fmt(v_se), _fmt(v_ppv)]
        ], columns=['', 'Q Se', 'Q +P', 'V Se', 'V +P'])
        final_path = outdir / 'final_report.csv'
        report_df.to_csv(final_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ æœ€ç»ˆæŠ¥å‘Šï¼š{final_path}')

        # ç”Ÿæˆå¯è¿½æº¯è¯¦æƒ… final_report_details.csvï¼ˆç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼›è®°å½•å®¹å·®/åˆå¹¶å¼€å…³ï¼‰
        def _fmt_pct(x: float) -> str:
            return f"{x * 100:.2f}"

        details_rows = [
            ['Run', 'Scope', 'MIT-BIH'],
            ['Run', 'ComparatorTolerance', 'default'],
            ['Run', 'MergeQRS', str(args.merge_qrs)],
            ['Q', 'TP', str(TP_N_sum)],
            ['Q', 'FP', str(FP_N_sum)],
            ['Q', 'FN', str(FN_N_sum)],
            ['Q', 'Se', _fmt_pct(q_se)],
            ['Q', 'PPV', _fmt_pct(q_ppv)],
            ['V', 'TP', str(TP_V_sum)],
            ['V', 'FP', str(FP_V_sum)],
            ['V', 'FN', str(FN_V_sum)],
            ['V', 'Se', _fmt_pct(v_se)],
            ['V', 'PPV', _fmt_pct(v_ppv)],
        ]
        details_df = pd.DataFrame(details_rows, columns=['Section', 'Item', 'Value'])
        details_path = outdir / 'final_report_details.csv'
        details_df.to_csv(details_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ è¯¦æƒ…æŠ¥å‘Šï¼š{details_path}')
    except Exception as e:
        (outdir / 'final_report_error.txt').write_text(str(e), encoding='utf-8')
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
