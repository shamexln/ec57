r"""
bxb_runner.py â€” ä½¿ç”¨ WFDB å®˜æ–¹æ¯”è¾ƒå™¨ bxb.exe è¿›è¡Œé€æ‹å¯¹æ¯”ï¼ˆBeat-by-beat comparisonï¼‰

åŠŸèƒ½ï¼š
- æ‰¹é‡ï¼ˆ--all æˆ– --recordsï¼‰ä¸ºæ¯æ¡è®°å½•ç¡®ä¿ç”Ÿæˆç®—æ³• CSV ä¸ WFDB æ³¨é‡Š <record>.bioï¼›
- è‡ªåŠ¨è°ƒç”¨ bxb.exe ä¸å‚è€ƒæ³¨é‡Š atr é€æ‹å¯¹æ¯”ï¼›
- è§£æ bxb.exe çš„æ ‡å‡†è¾“å‡ºï¼ŒæŠ½å– TP/FP/FNï¼ˆåŒ¹é…/è¯¯æŠ¥/æ¼æŠ¥ï¼‰ç­‰å…³é”®ç»Ÿè®¡ï¼›
- å°†æ¯è®°å½•ä¸æ€»ä½“çš„ç»Ÿè®¡å†™å…¥ metrics_bxb\summary.csvã€‚

æ³¨æ„ï¼š
- è¢«æµ‹æ³¨é‡Šæ‰©å±•å›ºå®šä¸º bioï¼ˆä¸ wfdb_runner.py çš„ --to-ann è¾“å‡ºä¸€è‡´ï¼‰ã€‚
- å‚è€ƒæ‰©å±•å›ºå®šä¸º atrï¼›å®¹å·®ç”± --tolerance-ms æŒ‡å®šï¼ˆé»˜è®¤ 100 msï¼‰ã€‚
- éœ€è¦å·²å®‰è£… wfdbï¼ˆä»…ç”¨äºè¯»å– fsï¼Œç”Ÿæˆ .bio æ—¶ä½¿ç”¨ï¼‰ã€‚
"""

from __future__ import annotations

import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import wfdb


def list_records(db_dir: Path, records_cli: List[str] | None, use_all: bool) -> List[str]:
    if records_cli:
        return [str(r) for r in records_cli]
    if use_all:
        return sorted([p.stem for p in db_dir.glob('*.hea')])
    raise SystemExit('å¿…é¡»ä½¿ç”¨ --records æˆ– --all æŒ‡å®šè®°å½•')


def rd_fs(db_dir: Path, record: str) -> float:
    sig, hdr = wfdb.rdsamp(str((db_dir / record).as_posix()))
    return float(hdr['fs'])


def csv_to_bio(record: str, csv_path: Path, out_dir: Path, fs: float) -> Path:
    """æŠŠç®—æ³• CSV è½¬ä¸º WFDB æ³¨é‡Š <record>.bioï¼ˆå…¼å®¹ä¸¤ç§åˆ—æ ¼å¼ï¼‰ã€‚
    åˆ—æ ¼å¼Aï¼ˆæ— è¡¨å¤´ï¼‰ï¼šindex,time,beat,hrï¼ˆé¦–è¡Œæ ‡é¢˜ï¼Œskiprows=1ï¼‰ã€‚
    åˆ—æ ¼å¼Bï¼ˆæœ‰è¡¨å¤´ï¼‰ï¼šIndexPos,time,beat type,heart rateã€‚
    ä½¿ç”¨ IndexPos ä½œä¸ºæ ·æœ¬ç´¢å¼•
    æœªè¯†åˆ« beat é»˜è®¤æ˜ å°„ä¸º 'N'ï¼›
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    bio_path = out_dir / f'{record}.bio'

    # å°è¯•æœ‰è¡¨å¤´
    def _normalize(cols: List[str]) -> List[str]:
        return [str(c).strip().lower().replace(' ', '') for c in cols]

    df = None
    try:
        df0 = pd.read_csv(csv_path)
        cols = _normalize(list(df0.columns))
        df0.columns = cols
        df = df0
    except Exception:
        df = None

    samples: np.ndarray
    beats: List[str]
    if df is None or df.empty or not set(df.columns).intersection({'indexpos', 'time', 'beat', 'beattype'}):
        # æ— è¡¨å¤´æ ¼å¼
        df = pd.read_csv(csv_path, header=None, names=['index', 'time', 'beat', 'hr'], skiprows=1)
        if df.empty:
            # å†™ä¸€ä¸ªç©ºæ³¨é‡Šæ–‡ä»¶ï¼ˆä»¥å… bxb æŠ¥é”™ï¼‰ï¼›ä½†é€šå¸¸è¿™æ„å‘³ç€ç®—æ³•æ— è¾“å‡º
            wfdb.wrann(record, 'bio', sample=np.asarray([], dtype=np.int64), symbol=[])
            return bio_path
        idx = pd.to_numeric(df['indexpos'], errors='coerce').to_numpy()
        samples = np.rint(idx).astype(np.int64)
        beats = df['beat'].astype(str).tolist()
    else:
        if 'indexpos' in df.columns:
            idx = pd.to_numeric(df['indexpos'], errors='coerce').to_numpy()
            samples = np.rint(idx).astype(np.int64)
        else:
            raise ValueError(f'{csv_path} ç¼ºå°‘ IndexPos æˆ– time åˆ—')
        bc = 'beat' if 'beat' in df.columns else ('beattype' if 'beattype' in df.columns else None)
        if bc is None:
            raise ValueError(f'{csv_path} ç¼ºå°‘ beat / beat type åˆ—')
        beats = df[bc].astype(str).tolist()

    # å°† L åŸæ ·æˆ–æ˜ å°„ï¼Ÿè¿™é‡Œä¸æ”¹å˜ labelï¼Œä»…ä½œä¸ºäº‹ä»¶ï¼›bxb åªå…³å¿ƒæ—¶é—´å¯¹é½
    symbols = beats

    # æ’åºå¹¶å†™å‡ºï¼ˆæ˜ç¡®å†™å…¥ bio_path æŒ‡å®šç›®å½•ï¼‰
    order = np.argsort(samples)
    samples = samples[order]
    symbols = [symbols[i] for i in order]
    # wfdb.wrann ä¼šä»¥å½“å‰å·¥ä½œç›®å½•ä¸ºåŸºå‡†å°† <record>.<ext> å†™å…¥ç£ç›˜ã€‚
    # ä¸ºç¡®ä¿å†™åˆ° bio_path æŒ‡å®šç›®å½•ï¼Œåˆ‡æ¢ä¸´æ—¶å·¥ä½œç›®å½•å†å†™å…¥ã€‚
    import os
    old_cwd = os.getcwd()
    try:
        os.chdir(str(out_dir))
        wfdb.wrann(record, 'bio', sample=samples, symbol=symbols)
    finally:
        os.chdir(old_cwd)
    return bio_path


def run_bxb(bxb_exe: Path, workdir: Path, record: str, wfdbpath: str, test_ext: str = 'bio') -> Tuple[str, int]:
    """è°ƒç”¨ bxb.exeï¼Œè¿”å›æ ‡å‡†è¾“å‡ºæ–‡æœ¬ä¸é€€å‡ºç ã€‚
    ä¼šåœ¨å­è¿›ç¨‹ç¯å¢ƒä¸­è®¾ç½® WFDBPATHï¼Œä»¥ä¾¿ bxb èƒ½æ‰¾åˆ° .hea/.dat/.atr ä¸ æµ‹è¯•æ³¨é‡Šï¼ˆæ‰©å±•åç”± test_ext æŒ‡å®šï¼‰ã€‚
    ä¾‹å¦‚ï¼štest_ext='nbio' æˆ– 'vbio'ã€‚
    """
    import os
    cmd = [str(bxb_exe), '-r', record, '-a', 'atr', str(test_ext)]
    env = os.environ.copy()
    env['WFDB'] = wfdbpath
    proc = subprocess.run(cmd, cwd=workdir, capture_output=True, text=True, encoding='utf-8', errors='replace', env=env)
    stdout = proc.stdout + (proc.stderr or '')
    return stdout, proc.returncode


def _read_bio_as_df(workdir: Path, record: str) -> pd.DataFrame:
    """è¯»å– workdir ä¸‹ <record>.bio ä¸º DataFrame(sample, symbol)ã€‚è‹¥ä¸å­˜åœ¨è¿”å›ç©º DFã€‚"""
    bio_path = workdir / f'{record}.bio'
    if not bio_path.exists():
        return pd.DataFrame({'sample': pd.Series(dtype='int64'), 'symbol': pd.Series(dtype='object')})
    ann = wfdb.rdann(str((workdir / record).as_posix()), 'bio')
    df = pd.DataFrame({'sample': np.asarray(ann.sample, dtype=np.int64), 'symbol': list(ann.symbol)})
    # æ’åºï¼Œå»é™¤ NaN/ç©ºç¬¦å·
    df = df.dropna(subset=['sample', 'symbol'])
    df = df.sort_values('sample').reset_index(drop=True)
    return df


def _write_bio_from_df(workdir: Path, record: str, df: pd.DataFrame, out_ext: str) -> Path:
    """å°† DataFrame å†™ä¸º WFDB æ³¨é‡Š <record>.<out_ext> åˆ° workdirã€‚"""
    samples = np.asarray(pd.to_numeric(df['sample'], errors='coerce').dropna().round().astype(np.int64)) if not df.empty else np.asarray([], dtype=np.int64)
    symbols = df['symbol'].astype(str).tolist() if not df.empty else []
    import os
    old_cwd = os.getcwd()
    try:
        os.chdir(str(workdir))
        wfdb.wrann(record, out_ext, sample=samples, symbol=symbols)
    finally:
        os.chdir(old_cwd)
    return workdir / f'{record}.{out_ext}'


def parse_bxb_stdout(text: str) -> Dict[str, int | float]:
    """ä» bxb è¾“å‡ºä¸­æŠ½å–å…³é”®ç»Ÿè®¡ã€‚ä¸åŒç‰ˆæœ¬æ ¼å¼ç•¥æœ‰åŒºåˆ«ï¼Œè¿™é‡Œä½¿ç”¨å…³é”®å­—æå–ï¼š
    è¿”å›é”®ï¼šTPï¼ˆåŒ¹é…ï¼‰ã€FPï¼ˆè¯¯æŠ¥ï¼‰ã€FNï¼ˆæ¼æŠ¥ï¼‰ã€TOTAL_REFã€TOTAL_TESTã€‚
    è‹¥æ— æ³•è¯†åˆ«ï¼Œè¿”å›ç©ºå­—å…¸ã€‚"""
    # ç®€å•å¯å‘å¼è§£æ
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    stats: Dict[str, int | float] = {}
    for ln in lines:
        low = ln.lower()
        # å¸¸è§å…³é”®å­—ï¼šmatched / false positives / false negatives / total annotated / total detected
        if 'matched' in low and any(ch.isdigit() for ch in low):
            # e.g., "Matched annotations: 2274"
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TP'] = int(digits[-1])
        elif 'false positive' in low:
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['FP'] = int(digits[-1])
        elif 'false negative' in low:
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['FN'] = int(digits[-1])
        elif ('total' in low and 'reference' in low) or ('total' in low and 'annotated' in low):
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TOTAL_REF'] = int(digits[-1])
        elif ('total' in low and 'test' in low) or ('total' in low and 'detected' in low):
            digits = ''.join(ch if ch.isdigit() else ' ' for ch in ln).split()
            if digits:
                stats['TOTAL_TEST'] = int(digits[-1])
    return stats


def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description='ä½¿ç”¨ bxb.exe å¯¹ç®—æ³• .bio ä¸å‚è€ƒ .atr åšé€æ‹å¯¹æ¯”ï¼ˆæ‰¹é‡ï¼‰')
    p.add_argument('--db', required=True, help='MIT/PhysioNet æ•°æ®ç›®å½•ï¼Œå¦‚ C\\project\\mit-database')
    p.add_argument('--exe', default='.\\ECGAlg.exe', help='ECGAlg.exe è·¯å¾„ï¼ˆç”¨äºç”Ÿæˆ CSV/.bioï¼‰')
    p.add_argument('--bxb', required=True, help='bxb.exe è·¯å¾„ï¼Œä¾‹å¦‚ C\\project\\wfdb-10.7.0\\build\\bin\\bxb.exe')
    p.add_argument('--workdir', default='.', help='å·¥ä½œç›®å½•ï¼ˆç”Ÿæˆ CSV/.bio ä¸æ‰§è¡Œ bxb çš„ä½ç½®ï¼‰')
    p.add_argument('--outdir', default='.\\metrics_bxb', help='è¾“å‡ºç›®å½•ï¼ˆsummary.csvï¼‰')
    p.add_argument('--records', nargs='*', help='æŒ‡å®šè®°å½•åˆ—è¡¨ï¼Œå¦‚ 100 101 103')
    p.add_argument('--all', action='store_true', help='å¯¹ç›®å½•ä¸‹æ‰€æœ‰ .hea è®°å½•æ‰§è¡Œ')
    p.add_argument('--tolerance-ms', type=int, default=100, help='bxb å®¹å·®ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 100')
    p.add_argument('--force', action='store_true', help='å³ä½¿å·²æœ‰ CSV/.bio ä¹Ÿå¼ºåˆ¶é‡è·‘ç®—æ³•ä¸é‡å†™ .bio')
    # QRS åˆå¹¶å¼€å…³ï¼šé»˜è®¤å¼€å¯ï¼Œå¯ç”¨ --no-merge-qrs å…³é—­
    p.add_argument('--merge-qrs', dest='merge_qrs', action='store_true', help="å°† QRS å­ç±» {'N','R','L','e','j','A','a','J','S'} åˆå¹¶ä¸º 'N'ï¼ˆç”¨äº nbio ç”Ÿæˆï¼‰")
    p.add_argument('--no-merge-qrs', dest='merge_qrs', action='store_false', help='å…³é—­ QRS å­ç±»åˆå¹¶ï¼Œä»…ä¿ç•™åŸå§‹ N ä½œä¸º Q åˆ—')
    p.set_defaults(merge_qrs=True)
    return p.parse_args(argv)


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def main(argv: List[str] | None = None) -> int:
    args = parse_args(argv)
    db_dir = Path(args.db)
    bxb_exe = Path(args.bxb)
    workdir = Path(args.workdir)
    outdir = Path(args.outdir)
    ensure_dir(outdir)
    ensure_dir(workdir)

    # å°† MIT æ•°æ®ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä¸å«å­ç›®å½•ï¼‰å¤åˆ¶åˆ° workdirï¼Œ
    # ä»¥ä¾¿ bxb åœ¨å·¥ä½œç›®å½•ç›´æ¥æ‰¾åˆ° <record>.hea/.dat/.atr
    # æ³¨æ„ï¼šåªå¤åˆ¶æ–‡ä»¶ï¼Œç›®å½•ä¸å¤åˆ¶ã€‚
    try:
        for p in db_dir.iterdir():
            if p.is_file():
                dst = workdir / p.name
                if not dst.exists():
                    dst.write_bytes(p.read_bytes())
    except Exception as e:
        print(f"âš  å¤åˆ¶ MIT æ•°æ®æ–‡ä»¶åˆ°å·¥ä½œç›®å½•æ—¶å‡ºé”™ï¼š{e}")

    records = list_records(db_dir, args.records, args.all)
    rows: List[Dict] = []

    # é€æ¡è®°å½•å¤„ç†
    for rec in records:
        try:
            fs = rd_fs(db_dir, rec)
            csv_path = workdir / f'{rec}.csv'
            bio_path = workdir / f'{rec}.bio'

            # è‹¥ç¼º CSV æˆ–å¼ºåˆ¶ï¼Œå…ˆç”¨ç°æœ‰è¿è¡Œå™¨äº§ç”Ÿ CSV
            if args.force or (not csv_path.exists()):
                # å¤ç”¨ wfdb_runner.py ä»¥ä¿æŒä¸€è‡´
                runner = Path(__file__).parent / 'wfdb_runner.py'
                cmd = [
                    'python', str(runner), '--db', str(db_dir), '--record', rec,
                    '--exe', str(Path(args.exe)), '--workdir', str(workdir), '--outdir', str(workdir), '--leadnum', '12'
                ]
                subprocess.run(cmd, check=True)

            # ç”Ÿæˆ .bioï¼ˆè‹¥ç¼ºæˆ–å¼ºåˆ¶ï¼‰ã€‚è‹¥ db_dir ä¸‹å·²æœ‰åŒå .bioï¼Œä¹Ÿä¼šå¤åˆ¶åˆ° workdirã€‚
            if not bio_path.exists():
                src_bio_in_db = db_dir / f'{rec}.bio'
                if src_bio_in_db.exists():
                    bio_path.write_bytes(src_bio_in_db.read_bytes())
                elif args.force or (not bio_path.exists()):
                    csv_to_bio(rec, csv_path, workdir, fs)
            elif args.force:
                # å¼ºåˆ¶é‡å†™
                csv_to_bio(rec, csv_path, workdir, fs)

            # åœ¨è°ƒç”¨ bxb å‰æ˜¾å¼æ£€æŸ¥ .bio æ˜¯å¦å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨åˆ™æŠ¥é”™å¹¶ç»™å‡ºæç¤º
            bio_exists = bio_path.exists()
            if not bio_exists:
                rows.append({'record': rec, 'exit_code': -2, 'error': f'ç¼ºå°‘ {bio_path}ï¼Œè¯·ç¡®è®¤å·²ç”Ÿæˆ CSV æˆ–æä¾›ç°æˆ .bio'})
                continue

            # è°ƒç”¨ bxb.exeï¼Œå¹¶æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆå­è¿›ç¨‹ cwd ä¸ WFDBPATH ä¼šè¢«è®¾ç½®ï¼‰
            wfdbpath = f"{db_dir};{workdir}"
            print(f"[bxb] cwd={workdir}")
            # å…ˆåŸºäºåŸå§‹ bio ç”Ÿæˆè¿‡æ»¤åçš„ nbio/vbio
            bio_df = _read_bio_as_df(workdir, rec)
            # æ ¹æ®å¼€å…³å†³å®š QRS åˆå¹¶é›†åˆ
            if args.merge_qrs:
                QRS_SET = {'N', 'R', 'L', 'e', 'j', 'A', 'a', 'J', 'S'}
                df_N = bio_df[bio_df['symbol'].isin(QRS_SET)].copy()
                if not df_N.empty:
                    df_N.loc[:, 'symbol'] = 'N'
            else:
                df_N = bio_df[bio_df['symbol'] == 'N'].copy()
            df_V = bio_df[bio_df['symbol'] == 'V'].copy()
            nbio_path = _write_bio_from_df(workdir, rec, df_N, 'nbio')
            vbio_path = _write_bio_from_df(workdir, rec, df_V, 'vbio')

            # 1) æ•´ä½“ç»Ÿè®¡ï¼ˆä¸åŒºåˆ†ç±»åˆ«ï¼Œæµ‹è¯•æ‰©å±•ä½¿ç”¨åŸ bioï¼‰
            stdout_all, code_all = run_bxb(bxb_exe, workdir, rec, wfdbpath=wfdbpath, test_ext='bio')
            stats_all = parse_bxb_stdout(stdout_all)

            # 2) QRSï¼ˆQï¼‰å¯¹åº” N ç±»ç»Ÿè®¡ï¼šç”¨ nbio
            stdout_N, code_N = run_bxb(bxb_exe, workdir, rec, wfdbpath=wfdbpath, test_ext='nbio')
            stats_N = parse_bxb_stdout(stdout_N)

            # 3) VEBï¼ˆVï¼‰å¯¹åº” V ç±»ç»Ÿè®¡ï¼šç”¨ vbio
            stdout_V, code_V = run_bxb(bxb_exe, workdir, rec, wfdbpath=wfdbpath, test_ext='vbio')
            stats_V = parse_bxb_stdout(stdout_V)

            row: Dict[str, int | float | str] = {'record': rec, 'exit_code': code_all}

            # æ•´ä½“æŒ‡æ ‡åˆ—ï¼ˆä¿æŒå…¼å®¹ï¼‰
            TP = int(stats_all.get('TP', 0)); FP = int(stats_all.get('FP', 0)); FN = int(stats_all.get('FN', 0))
            se = TP / (TP + FN) if (TP + FN) > 0 else 0.0
            ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0
            row.update({'TP': TP, 'FP': FP, 'FN': FN, 'Se': se, 'PPV': ppv})

            # N ç±»ï¼ˆQï¼‰æŒ‡æ ‡
            TP_N = int(stats_N.get('TP', 0)); FP_N = int(stats_N.get('FP', 0)); FN_N = int(stats_N.get('FN', 0))
            se_N = TP_N / (TP_N + FN_N) if (TP_N + FN_N) > 0 else 0.0
            ppv_N = TP_N / (TP_N + FP_N) if (TP_N + FP_N) > 0 else 0.0
            row.update({'TP_N': TP_N, 'FP_N': FP_N, 'FN_N': FN_N, 'Se_N': se_N, 'PPV_N': ppv_N})

            # V ç±»æŒ‡æ ‡
            TP_V = int(stats_V.get('TP', 0)); FP_V = int(stats_V.get('FP', 0)); FN_V = int(stats_V.get('FN', 0))
            se_V = TP_V / (TP_V + FN_V) if (TP_V + FN_V) > 0 else 0.0
            ppv_V = TP_V / (TP_V + FP_V) if (TP_V + FP_V) > 0 else 0.0
            row.update({'TP_V': TP_V, 'FP_V': FP_V, 'FN_V': FN_V, 'Se_V': se_V, 'PPV_V': ppv_V})

            # è®°å½•å„è‡ªçš„é€€å‡ºç ä¾¿äºæ’é”™
            row.update({'exit_code_N': code_N, 'exit_code_V': code_V, 'stdout_N': stdout_N, 'stdout_V': stdout_V})

            rows.append(row)
        except Exception as e:
            rows.append({'record': rec, 'error': str(e), 'exit_code': -1})

    # å†™å‡ºæ±‡æ€»
    df = pd.DataFrame(rows)
    df = df.sort_values('record')
    # å…ˆå†™å‡ºåŸå§‹æ˜ç»†
    (outdir / 'summary.csv').write_text(df.to_csv(index=False), encoding='utf-8')
    print(f'ğŸ“„ bxb æ±‡æ€»å†™å…¥ï¼š{outdir / "summary.csv"}')

    # åœ¨ summary.csv ä¸­è¿½åŠ  TOTAL è¡Œï¼ˆåˆè®¡ï¼‰
    try:
        def _num_series(name: str) -> pd.Series:
            return pd.to_numeric(df.get(name, pd.Series(dtype=float)), errors='coerce').fillna(0)

        TP_sum = int(_num_series('TP').sum())
        FP_sum = int(_num_series('FP').sum())
        FN_sum = int(_num_series('FN').sum())
        Se_sum = (TP_sum / (TP_sum + FN_sum)) if (TP_sum + FN_sum) > 0 else 0.0
        PPV_sum = (TP_sum / (TP_sum + FP_sum)) if (TP_sum + FP_sum) > 0 else 0.0

        TP_N_sum = int(_num_series('TP_N').sum())
        FP_N_sum = int(_num_series('FP_N').sum())
        FN_N_sum = int(_num_series('FN_N').sum())
        Se_N_sum = (TP_N_sum / (TP_N_sum + FN_N_sum)) if (TP_N_sum + FN_N_sum) > 0 else 0.0
        PPV_N_sum = (TP_N_sum / (TP_N_sum + FP_N_sum)) if (TP_N_sum + FP_N_sum) > 0 else 0.0

        TP_V_sum = int(_num_series('TP_V').sum())
        FP_V_sum = int(_num_series('FP_V').sum())
        FN_V_sum = int(_num_series('FN_V').sum())
        Se_V_sum = (TP_V_sum / (TP_V_sum + FN_V_sum)) if (TP_V_sum + FN_V_sum) > 0 else 0.0
        PPV_V_sum = (TP_V_sum / (TP_V_sum + FP_V_sum)) if (TP_V_sum + FP_V_sum) > 0 else 0.0

        total_row = {
            'record': 'TOTAL',
            'TP': TP_sum, 'FP': FP_sum, 'FN': FN_sum, 'Se': Se_sum, 'PPV': PPV_sum,
            'TP_N': TP_N_sum, 'FP_N': FP_N_sum, 'FN_N': FN_N_sum, 'Se_N': Se_N_sum, 'PPV_N': PPV_N_sum,
            'TP_V': TP_V_sum, 'FP_V': FP_V_sum, 'FN_V': FN_V_sum, 'Se_V': Se_V_sum, 'PPV_V': PPV_V_sum,
        }
        df2 = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)
        (outdir / 'summary.csv').write_text(df2.to_csv(index=False), encoding='utf-8')
        print('â• å·²åœ¨ summary.csv è¿½åŠ  TOTAL åˆè®¡è¡Œ')
    except Exception as e:
        print(f'âš  è¿½åŠ  TOTAL è¡Œå¤±è´¥ï¼š{e}')

    # ç”Ÿæˆâ€œæœ€ç»ˆæŠ¥å‘Šâ€ï¼šæŒ‰ bxb -l N ä¸ -l V åˆ†åˆ«ç»Ÿè®¡çš„ç»“æœæ±‡æ€»ï¼Œ
    # è¾“å‡ºåˆ° outdir/final_report.csvï¼Œè¡¨å¤´ä¸ eval_atr_metrics.py ä¸€è‡´ï¼š
    # "", Q Se, Q +P, V Se, V +P
    try:
        # æ±‡æ€» N ç±»ï¼ˆå¯¹åº” Q åˆ—ï¼‰
        TP_N_sum = int(pd.to_numeric(df.get('TP_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FP_N_sum = int(pd.to_numeric(df.get('FP_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FN_N_sum = int(pd.to_numeric(df.get('FN_N', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())

        q_se = (TP_N_sum / (TP_N_sum + FN_N_sum)) if (TP_N_sum + FN_N_sum) > 0 else 0.0
        q_ppv = (TP_N_sum / (TP_N_sum + FP_N_sum)) if (TP_N_sum + FP_N_sum) > 0 else 0.0

        # æ±‡æ€» V ç±»
        TP_V_sum = int(pd.to_numeric(df.get('TP_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FP_V_sum = int(pd.to_numeric(df.get('FP_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())
        FN_V_sum = int(pd.to_numeric(df.get('FN_V', pd.Series(dtype=float)), errors='coerce').fillna(0).sum())

        v_se = (TP_V_sum / (TP_V_sum + FN_V_sum)) if (TP_V_sum + FN_V_sum) > 0 else 0.0
        v_ppv = (TP_V_sum / (TP_V_sum + FP_V_sum)) if (TP_V_sum + FP_V_sum) > 0 else 0.0

        def _fmt(x: float) -> str:
            return f"{x * 100:.2f}"

        report_df = pd.DataFrame([
            ['MIT-BIH', _fmt(q_se), _fmt(q_ppv), _fmt(v_se), _fmt(v_ppv)]
        ], columns=['', 'Q Se', 'Q +P', 'V Se', 'V +P'])
        final_path = outdir / 'final_report.csv'
        report_df.to_csv(final_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ æœ€ç»ˆæŠ¥å‘Šï¼š{final_path}')

        # ç”Ÿæˆå¯è¿½æº¯è¯¦æƒ… final_report_details.csvï¼ˆç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼›è®°å½•å®¹å·®/åˆå¹¶å¼€å…³ï¼‰
        def _fmt_pct(x: float) -> str:
            return f"{x * 100:.2f}"

        details_rows = [
            ['Run', 'Scope', 'MIT-BIH'],
            ['Run', 'ComparatorTolerance', 'default'],
            ['Run', 'MergeQRS', str(args.merge_qrs)],
            ['Q', 'TP', str(TP_N_sum)],
            ['Q', 'FP', str(FP_N_sum)],
            ['Q', 'FN', str(FN_N_sum)],
            ['Q', 'Se', _fmt_pct(q_se)],
            ['Q', 'PPV', _fmt_pct(q_ppv)],
            ['V', 'TP', str(TP_V_sum)],
            ['V', 'FP', str(FP_V_sum)],
            ['V', 'FN', str(FN_V_sum)],
            ['V', 'Se', _fmt_pct(v_se)],
            ['V', 'PPV', _fmt_pct(v_ppv)],
        ]
        details_df = pd.DataFrame(details_rows, columns=['Section', 'Item', 'Value'])
        details_path = outdir / 'final_report_details.csv'
        details_df.to_csv(details_path, index=False, encoding='utf-8')
        print(f'ğŸ“„ è¯¦æƒ…æŠ¥å‘Šï¼š{details_path}')
    except Exception as e:
        (outdir / 'final_report_error.txt').write_text(str(e), encoding='utf-8')
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
